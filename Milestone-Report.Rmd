---
title: "Milestone Report"
subtitle: "Data Science - Capstone [Coursera]"
author: "Dr. Chamika Senanayake"
date: "10/12/2020"
output: 
  html_document:
    toc: yes
    toc_float: yes
    theme: cosmo
    keep_md: no
    df_print: paged
    css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsys

the training data to get  started that will be the basis for most of the capstone. we must download the data from the Coursera site and not from external websites to start.The goal of this task is to get familiar with the databases and do the necessary cleaning. the first thing is to understand the language and its peculiarities with respect to your target. You can learn to read, speak and write the language. Alternatively, you can study data and learn from existing information about the language through literature and the internet. At the very least, you need to understand how the language is written: writing script, existing input methods, some phonetic knowledge, etc.

This milestone report requires to download the dataset, cleaning the dataset & do an exploratory analysis of the data set

## Data Acquicision 

### Setting environment 

````{r}
library(downloader)
library(plyr);
library(dplyr)
library(knitr)
library(tm)
````

### Step 1: Download the dataset and unzip folder
````{r}
if(!file.exists("./projectData")){
  dir.create("./projectData")
}
Url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

if(!file.exists("./projectData/Coursera-SwiftKey.zip")){
  download.file(Url,destfile="./projectData/Coursera-SwiftKey.zip",mode = "wb")
}
if(!file.exists("./projectData/final")){
  unzip(zipfile="./projectData/Coursera-SwiftKey.zip",exdir="./projectData")
}
``` 

since the data set is so large we have to read it line by line only the amount of data required. before doing that, visualization and listing of all files int eh directory is done

the main data set consist of data from 3 sources
1. News
2. Blogs 
3. Twitter feeds.

for the purpose of this capstone, only english_US dataset is emphasized.

```{r}
path <- file.path("./projectData/final" , "en_US")
files<-list.files(path, recursive=TRUE)
# file connection of the twitter data set
con <- file("./projectData/final/en_US/en_US.twitter.txt", "r") 
#lineTwitter<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineTwitter<-readLines(con, skipNul = TRUE)
# Close the connection
close(con)
# file connection of the blog data set
con <- file("./projectData/final/en_US/en_US.blogs.txt", "r") 
#lineBlogs<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineBlogs<-readLines(con, skipNul = TRUE)
# Close the connection handle
close(con)
# file connection of the news data set
con <- file("./projectData/final/en_US/en_US.news.txt", "r") 
#lineNews<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineNews<-readLines(con, skipNul = TRUE)
# Close the connection handle when you are done
close(con)
```

quick examination of datasets and summerized by file sizes, line count, word counts and mean words per line as below.

```{r}
library(stringi)
#file sizes
lineBlogs.size <- file.info("./projectData/final/en_US/en_US.blogs.txt")$size / 1024 ^ 2
lineNews.size <- file.info("./projectData/final/en_US/en_US.news.txt")$size / 1024 ^ 2
lineTwitter.size <- file.info("./projectData/final/en_US/en_US.twitter.txt")$size / 1024 ^ 2
#words in files
lineBlogs.words <- stri_count_words(lineBlogs)
lineNews.words <- stri_count_words(lineNews)
lineTwitter.words <- stri_count_words(lineTwitter)
# Dataset sumerization
data.frame(source = c("blogs", "news", "twitter"),
           file.size.MB = c(lineBlogs.size, lineNews.size, lineTwitter.size),
           num.lines = c(length(lineBlogs), length(lineNews), length(lineTwitter)),
           num.words = c(sum(lineBlogs.words), sum(lineNews.words), sum(lineTwitter.words)),
           mean.num.words = c(mean(lineBlogs.words), mean(lineNews.words), mean(lineTwitter.words)))
```

## Data Cleaning

This involves removing URLs, special characters, punctuations, numbers, excess whitespace, stopwords, and changing the text to lower case. Since the data sets are quite large, randomly choosen 2% of the data to demonstrate the data cleaning and exploratory analysis also please take care of the UTF chars.

```{r}
library(tm)
# data sampling
set.seed(5000)
data.sample <- c(sample(lineBlogs, length(lineBlogs) * 0.02),
                 sample(lineNews, length(lineNews) * 0.02),
                 sample(lineTwitter, length(lineTwitter) * 0.02))
#corpus and clean the data
corpus <- VCorpus(VectorSource(data.sample))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
```

## Exploratory Data Analysis

we list the most common (n-grams) uni-grams, bi-grams, and tri-grams.

```{r}
library(RWeka)
library(ggplot2)
# annotate
options(mc.cores=1)
# frequencies of the word
getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
makePlot <- function(data, label) {
  ggplot(data[1:30,], aes(reorder(word, -freq), freq)) +
         labs(x = label, y = "Frequency") +
         theme(axis.text.x = element_text(angle = 60, size = 12, hjust = 1)) +
         geom_bar(stat = "identity", fill = I("blue"))
}
#frequencies of most common n-grams in data sample
freq1 <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus), 0.9999))
freq2 <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = bigram)), 0.9999))
freq3 <- getFreq(removeSparseTerms(TermDocumentMatrix(corpus, control = list(tokenize = trigram)), 0.9999))
```

histogram of the 30 most common **unigrams** in the data sample.

```{r}
makePlot(freq1, "30 Most Common Uni-grams")
```

histogram of the 30 most common **bigrams** in the data sample.

```{r}
makePlot(freq2, "30 Most Common Bi-grams")
```

histogram of the 30 most common **trigrams** in the data sample.

```{r}
makePlot(freq3, "30 Most Common Tri-grams")
```

## Conclusion & plan of action.

1. Prediction

We see that small parts of the data are responsible for the bulk of the corpus. This allows prediction to be a smaller model to just focus on the most important parts.

2. Next steps

Reevaluate approach and see if sample size adjust,inclusion of stopwords, punctuation, numbers, etc improve prediction
Building a predictive model using the identified tokens
Wrapping up the results and the developed model as a data product, shiny app.
